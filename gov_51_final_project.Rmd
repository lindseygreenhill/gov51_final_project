---
title: "gov_51_final_project"
author: "Owen Bernstein and Lindsey Greenhill"
date: "11/15/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Loading necessary packages

library(gt)
library(quanteda)
library(broom)
library(skimr)
library(lubridate)
library(janitor)
library(dotwhisker)
library(tidytext)
library(ggthemes)
library(webshot)
library(stargazer)
library(tidyverse)
library(patchwork)
library(ggrepel)
```

```{r data loading, include = FALSE}

# Reading in the data and then using bind rows to combine the candidates'
# Chyrons.

data <- read_csv("data/10-17-10-23.csv") %>%
  bind_rows(read_csv("data/10-24-10-31.csv")) %>%
  bind_rows(read_csv("data/11-1-11-7.csv")) %>%
  bind_rows(read_csv("data/11-8-11-14.csv")) %>%
  clean_names() %>%
  filter(channel != "BBCNEWS") %>%
  mutate(ranking = if_else(channel == "FOXNEWSW",
                           1, if_else(channel == "CNNW", 2, 3)))

# Cleaning the Chyrons and creating variables for post election and primetime
# coverage

tidy_data <- data %>% 
  clean_names() %>% 
  mutate(date = mdy_hm(date_time_utc)) %>%
  mutate(date_x = as.Date(substr(date, 1, 10)),
         hour = as.double(substr(date, 12, 13))) %>%
  select(channel, text, ranking, date_x, hour) %>% 
  mutate(post_election = ifelse(date_x > "2020-11-03", "Post-Election", "Pre-Election"),
         primetime = ifelse(hour > 19 & hour < 24, "Primetime", "Not Primetime"))

# Turning data into a corpus for quanteda

text_corpus <- corpus(tidy_data, text_field = "text")

```

```{r wordcloud, include = FALSE}

# Turning corpus into tokens. Removing unimportant words and selecting for
# ngrams = 2

wordcloud_toks <- tokens(text_corpus, 
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE,
                      remove_url = TRUE) %>% 
  tokens_tolower() %>%
  tokens_remove(pattern=stopwords("en")) %>% 
  tokens_remove(pattern= c("u201c", "u00b0", "u2014", "wopi", "avi", "ooo", "000", "ito", "ynl", "f'avl", "foxnews.com", "ufb02pi", "ufb021", "ufbo2l", "ufb02L", "rpm")) %>% 
  tokens_select(min_nchar=3) %>% 
  tokens_ngrams(n = 2)

wordcloud_dfm <- dfm(wordcloud_toks, groups = "channel")

textplot_wordcloud(wordcloud_dfm, comparison = T, min_count = 150)

```

```{r comparison graphs, include = FALSE}

# Using tokens to compare word usage by post_election variable

post_election_dfm <- dfm(wordcloud_toks, groups = "post_election")

post_election_keyness <- textstat_keyness(post_election_dfm, target = "Post-Election")

post_election_relative <- textplot_keyness(post_election_keyness, n = 15L,
                                           margin = .2,
                                           labelsize = 3)

# Using tokens to compare word usage by primetime variable

primetime_dfm <- dfm(wordcloud_toks, groups = "primetime")

primetime_keyness <- textstat_keyness(primetime_dfm, target = "Primetime")

primetime_relative <- textplot_keyness(primetime_keyness, n = 15L,
                                       margin = .2,
                                       labelsize = 3)

```

```{r dictionary and data frame, include = FALSE}

# Creating the content dictionaries

content_dict <- dictionary(list(populism = c("deceit", "treason",
                             "betray", "absurd",
                             "arrogant", "promise", 
                             "corrupt", "direct",
                             "elite", "establishment",
                             "ruling", "caste",
                             "class", "mafia",
                             "undemocratic", "politics", "political", "politicize", "politician",
                             "propaganda", "referendum",
                             "regime", "shame",
                             "admit", "tradition",
                             "people"),
                environment = c("green","climate",
                                "environment","heating",
                                "durable"),
                immigration = c("asylum","halal",
                                "scarf","illegal",
                                "immigrant", "immigration", "immigrate", "Islam", 
                                "Koran","Muslim",
                                "foreign"),
                progressive = c("progress","right",
                                "freedom","self-disposition",
                                "handicap","poverty",
                                "protection","honest",
                                "equal","education",
                                "pension","social",
                                "weak"),
                conservative = c("belief","famil",
                                 "church","norm",
                                 "porn","sex",
                                 "values","conservative",
                                 "conservatism","custom")))

# Creating new dfm for the content analysis

content_toks <- tokens(text_corpus, 
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE,
                      remove_url = TRUE) %>% 
  tokens_tolower() %>%
  tokens_remove(pattern=stopwords("en")) %>% 
  tokens_select(min_nchar=3)

content_dfm <- dfm(content_toks, groups = c("channel", "date_x"))

# Selecting words in the dictionaries

content_categories <- dfm_lookup(content_dfm, dictionary = content_dict)

# Turning dfm into dataframe

content_df <- convert(content_categories, to = "data.frame") %>% 
  group_by(doc_id) %>% 
  separate(doc_id, c("channel", "date"), extra = "merge") %>% 
  mutate(date = ymd(date))

```

```{r creating boxplots, include = FALSE}

# Creating boxplot for populism by channel

pop_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, populism)) +
  geom_boxplot() + 
  labs(title = "Populist Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

# Creating boxplot for environment by channel

env_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, environment)) +
  geom_boxplot() + 
  labs(title = "Environmental Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

# Creating boxplot for immigration by channel

img_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, immigration)) +
  geom_boxplot() + 
  labs(title = "Immigration Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

# Creating boxplot for progressive by channel

pro_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, progressive)) +
  geom_boxplot() + 
  labs(title = "Progressive Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

# Creating boxplot for conservative by channel

con_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, conservative)) +
  geom_boxplot() + 
  labs(title = "Conservative Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

```

```{r making gt table, include = FALSE}

# Making gt table of full counts

full_count_dfm <- dfm(content_toks, groups = c("channel"))

full_count_categories <- dfm_lookup(full_count_dfm, dictionary = content_dict)

full_count_df <- convert(full_count_categories, to = "data.frame") %>% 
  mutate(Channel = doc_id) %>% 
  select(-doc_id)

full_count_gt <- gt(full_count_df) %>% 
  tab_header(title = "Language Sentiment Across Channels",
             subtitle = "October 17 to November 14") %>% 
  cols_move(
    columns = vars(populism, environment, immigration, progressive, conservative),
    after = vars(Channel)
  ) %>% 
  tab_spanner(label = "Word Count for Each Content Category", columns = vars(populism, environment, immigration, progressive, conservative)) %>% 
  cols_label(populism = "Populism", environment = "Environment", immigration = "Immigration", 
             progressive = "Progressive", conservative = "Conservative")


```

```{r regressions, include = FALSE}

# Making dfm for regression and turning it into a df

regression_dfm <- dfm(content_toks, groups = c("channel", "post_election", "primetime", "date_x", "ranking"))

regression_categories <- dfm_lookup(regression_dfm, dictionary = content_dict)

regression_df <- convert(regression_categories, to = "data.frame") %>% 
  group_by(doc_id) %>% 
  separate(doc_id, c("channel", "election", "primetime", "date", "ranking"), sep = "([.])", extra = "merge") %>% 
  mutate(date = ymd(date))

# Running regressions

pop_fit <-
  lm(populism ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)

env_fit <-
  lm(environment ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)

img_fit <-
  lm(immigration ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)

pro_fit <-
  lm(progressive ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)

con_fit <-
  lm(conservative ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)
```

# Project Introduction

The purpose of this project is to examine what factors influence news coverage. We became interested in this general topic after hearing and reading about increasing media polarization between conservatives and liberals. Per work from the [Pew Research Center](https://www.journalism.org/2020/01/24/u-s-media-polarization-and-the-2020-election-a-nation-divided/), Americans are increasingly watching news channels that are seen as in line with their own political views, and avoiding those that are not. The result is a media industry that is, for the most part, divided by political ideology. With this in mind, we wanted to see if news channels that are seen as leaning left or right present meaningfully different news coverage. 

### Project Design

In order to assess potential differences in news coverage amongst channels, we used a data set created by the [Internet Archive's Third Eye Project](https://archive.org/services/third-eye.php). This data set contains news chyrons - the scrolling captions at the bottom of broadcast images - for four different news channels (MSNBC, CNN, Fox News, and BBC). Although it would be ideal to work with full transcripts of news broadcasts, we did not have access to that data, and we believe chryons are reasonably representative of each channel's coverage. We only included data from MSNBC, CNN, and Fox News, because the data for BBC was problematic in its transcription. The data includes coverage from October 17, 2020 to November 14, 2020. 

We performed textual analysis to classify the language each channel used into five categories: Populist, Environmental, Progressive, Conservative, and Immigration Related. We classified the language using preexisting dictionaries taken from a [2017 study](https://www.tandfonline.com/doi/abs/10.1080/17457289.2011.539483) of partisan language in Belgium. It would have been better to use dictionaries created for American politics, but we still think that the ones in the study are relevant and therefore useful. See the word basket section for a more details discussion on the dictionaries. 

We ran a linear regression to see if there was a statistically significant relationship between channel and language usage. We also included binary variables of whether or not the coverage was before the election and whether or not the coverage was in prime time in the regression. See the regression section for a discussion on why we thought these variables are important to include. 

### Initial Hypothesis

Our initial hypothesis is that Fox News, the most conservative of the news channels, will use more conservative, populist, and immigration related language while CNN and MSNBC will use more progressive and environmental related language. 

\newpage

# Exploratory Analysis

We used the quanteda package to perform initial textual analysis of the news chyrons. We also looked at the distributions of language usage across channels. Overall, our findings suggest that our initial hypothesis is incorrect. 

## Word Cloud Analysis

The graphic below shows a word cloud graphic that compares the language usage between Fox News, CNN, and MSNBC. In our analysis, we excluded filler words, punctuation, etc. and looked for two word phrases. Fox News is dark blue, CNN is light blue, and MSNBC is green. 

```{r wc, echo=FALSE}
textplot_wordcloud(wordcloud_dfm, comparison = T, 
  min_size = .5,
  max_size = 3.7,
  min_count = 150)
```

### Discussion

The most mentioned phrase amonst all the networks is President Trump (from Fox), which makes sense given how close our time frame is to the 2020 election. Fox also seemed to focus on Hunter Biden, Joe Biden, and democracy. CNN's most used phrase was "right now," which is perhaps an indication of the tone of their news coverage. CNN also mentions cities such as Chicago, Detroit, Houston, Miami, and a few more. MSNBC also mentions Trump often. It also used the phrases ""biden leads," "north carolina," "election day," and "covid cases" frequently.

\newpage

## Keyness Plot Analysis

In addition to word cloud analysis, we also looked at keyness plots of the news chyrons. A keyness plot is a plot that compares the usage of words between two different data sets. We wanted to look at the potential impact of our post_election and prime_time variables. We created a keyness plot for each variable. The first keyness plot looks at the difference in language use from all channels betwee pre and post election coverage. The second keness plot looks at the difference in language use from all channels between prime time and non prime time coverage. 

### Election Keyness Plot

The differences in language usage pre and post election are intuitive. Compared to after the election, news coverage focused much more on campaigns and voting processes. Compared to before the election, news coverage focused more on calling the election for Biden and the white house transition from Trump to Biden. Overall, there seems to be a shift in topics between pre election and post election coverage. 

```{r post_election_key, echo=FALSE}
post_election_relative
```
 \newpage
 
### Prime Time Keyness Plot

We thought it could be interesting to look at prime time coverage vs non prime time coverage because of the saliency of prime time coverage. Additionally, the respective  hosts for each channel's prime time news shows tend to be particulary divisive politically, so we thought that might have an impact on the language used during their shows. Interestingly, non prime time coverage seems to focus more on cities compared to prime time coverage. Meanwhile, prime time coverage seems to focus on the election compared to non primt etime coverage.


```{r ptime_key, echo=FALSE}
primetime_relative
```

\newpage

## Boxplot Analysis





## Preliminary Analysis

In this analysis, we gathered, cleaned, and wrangled news data from October 17 to November 14, 2020. 
We chose to include Fox News, CNN, and MSNB in our analysis and assigned those channels ideological
rankings of 1, 2, and 3, respectively. The plots below show our preliminary findings.

### How much specific language is each channel using?

Visualizing the language each channel uses by a variety of variables. 



We classified the below categories using word baskets built by the study referenced in our project proposal. The table below shows the raw counts of how many times each channel used a word associated with these categories. 

```{r gt, echo=FALSE}
full_count_gt
```

### How much specific language is each channel using each day?

The histograms below visualize how much channels use the categorical language on a daily basis. 

```{r hists, warning = FALSE, echo=FALSE}

pop_box
img_box  
env_box 
pro_box 
con_box

```

### How does usage vary across time?

The plots below visualize the change in daily language use for each channel.

```{r time, echo=FALSE, warning=FALSE}
#language_all_counts
```

### Regression: Does Ideology have an affect on language usage?

The table below shows the results of five different regressions and the plot below shows the line of best fit for each regression. 

```{r regs, results = "asis", echo=FALSE, warning=FALSE}
stargazer(pop_fit, img_fit,
          env_fit,
          pro_fit,
          con_fit,
          single.row = TRUE,
          column.sep.width = "0.3pt",
          font.size= "footnotesize",
          type = "latex")

```

There does not appear to be a strong relationship between ideology and language usage. 


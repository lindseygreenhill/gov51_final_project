---
title: "gov_51_final_project"
author: "Owen Bernstein and Lindsey Greenhill"
date: "11/15/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Loading necessary packages

library(gt)
library(broom)
library(skimr)
library(lubridate)
library(janitor)
library(dotwhisker)
library(tidytext)
library(ggthemes)
library(webshot)
library(stargazer)
library(tidyverse)

```

```{r data loading, include = FALSE}

# Reading in the data and then using bind rows to combine the candidates'
# speeches.

data <- read_csv("data/10-17-10-23.csv") %>%
  bind_rows(read_csv("data/10-24-10-31.csv")) %>%
  bind_rows(read_csv("data/11-1-11-7.csv")) %>%
  bind_rows(read_csv("data/11-8-11-14.csv")) %>%
  clean_names() %>%
  filter(channel != "BBCNEWS") %>%
  mutate(ranking = if_else(channel == "FOXNEWSW",
                           1, if_else(channel == "CNNW", 2, 3)))

# Cleaning the speeches and counting the number of words while removing stop
# words.

tidy_data <- data %>% 
  clean_names() %>% 
  mutate(date = mdy_hm(date_time_utc)) %>%
  mutate(date_x = as.Date(substr(date, 1, 10))) %>%
  select(channel, text, ranking, date_x) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  group_by(channel, ranking, date_x, word) %>%
  count()



```

```{r basket of words, include = FALSE}

# Counting words by each differnet content category

populism <- tidy_data %>% 
  group_by(date_x, channel, ranking) %>% 
  filter(str_detect(word, "deceit") | str_detect(word, "treason")
         | str_detect(word, "betray") | str_detect(word, "absurd")
         | str_detect(word, "arrogant") | str_detect(word, "promise") 
         | str_detect(word, "corrupt") | str_detect(word, "direct")
         | str_detect(word, "elite") | str_detect(word, "establishment")
         | str_detect(word, "ruling") | str_detect(word, "caste")
         | str_detect(word, "class") | str_detect(word, "mafia")
         | str_detect(word, "freedom of expression")
         | str_detect(word, "undemocratic") | str_detect(word, "politic")
         | str_detect(word, "propaganda") | str_detect(word, "referend")
         | str_detect(word, "regime") | str_detect(word, "shame")
         | str_detect(word, "admit") | str_detect(word, "tradition")
         | str_detect(word, "people")) %>% 
  summarize(populism_count = sum(n)) %>% 
  arrange(desc(populism_count))

environment <- tidy_data %>% 
  group_by(date_x, channel, ranking) %>% 
  filter(str_detect(word, "green") | str_detect(word, "climate")
         | str_detect(word, "environment") | str_detect(word, "heating")
         | str_detect(word, "durable")) %>% 
  summarise(environment_count = sum(n)) %>% 
  arrange(desc(environment_count))

immigration <- tidy_data %>% 
  group_by(date_x, channel, ranking) %>% 
  filter(str_detect(word, "asylum") | str_detect(word, "halal")
         | str_detect(word, "scarf") | str_detect(word, "illegal")
         | str_detect(word, "immigra") | str_detect(word, "Islam") 
         | str_detect(word, "Koran") | str_detect(word, "Muslim")
         | str_detect(word, "foreign")) %>% 
  summarise(immigration_count = sum(n)) %>% 
  arrange(desc(immigration_count))

progressive <- tidy_data %>% 
  group_by(date_x, channel, ranking) %>% 
  filter(str_detect(word,"progress") | str_detect(word, "right")
         | str_detect(word, "freedom") | str_detect(word, "self-disposition")
         | str_detect(word, "handicap") | str_detect(word, "poverty") 
         | str_detect(word, "protection") | str_detect(word, "honest")
         | str_detect(word, "equal") | str_detect(word, "education")
         | str_detect(word, "pension") | str_detect(word, "social") 
         | str_detect(word, "weak")) %>% 
  summarise(progressive_count = sum(n)) %>% 
  arrange(desc(progressive_count))

conservatism <- tidy_data %>% 
  group_by(date_x, channel, ranking) %>%  
  filter(str_detect(word, "belief") | str_detect(word, "famil")
         | str_detect(word, "church") | str_detect(word, "norm")
         | str_detect(word, "porn") | str_detect(word, "sex")
         | str_detect(word, "values") | str_detect(word, "conservative")
         | str_detect(word, "conservatism") | str_detect(word, "custom")) %>% 
  summarise(conservatism_count = sum(n)) %>% 
  arrange(desc(conservatism_count))

# Combining each of the conent categories into a single data frame. Also adding
# party affiliations and percent values

sentiment_channels <- populism %>% 
  full_join(environment, by = c("date_x", "channel",
                                "ranking")) %>% 
  full_join(immigration, by = c("date_x", "channel",
                                "ranking")) %>% 
  full_join(progressive, by = c("date_x", "channel",
                                "ranking")) %>% 
  full_join(conservatism, by = c("date_x", "channel",
                                "ranking"))%>%
  select(date_x, channel, ranking, populism_count,
         environment_count, immigration_count, 
         progressive_count, conservatism_count) %>%
  ungroup() %>% 
  distinct()

save(sentiment_channels, file = "data/sentiment_channels.Rdata")
```


```{r making graphs}

load("data/sentiment_channels.Rdata")

pop_time <- ggplot(sentiment_channels, aes(date_x, populism_count, color = channel)) +
  geom_jitter(alpha = 0.5, width = 100) +
  geom_smooth(method = "lm", se = F) +
  labs(title = "Populist Language in Presidential Speeches Over Time",
       x = "Date of Speech", y = "Percent Populist Language") +
  theme_minimal()


pop_hist <- sentiment_channels %>% 
  ggplot(., aes(channel, conservatism_count)) +
  geom_boxplot() + 
  labs(title = "Populist Language in Speeches by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Populist Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

language_all_counts <- sentiment_channels %>%
  pivot_longer(cols = populism_count:conservatism_count,
               names_to = "metric",
               values_to = "count") %>%
  ggplot(aes(x = date_x, y = count, color = channel)) +
  geom_point() +
  theme_classic() +
  facet_wrap(~metric) +
  labs(title = "Language Sentiment Across Channels",
       subtitle = "October 17 to November 14",
       x = "Date",
       y = "Word Count")

sum_table <- sentiment_channels %>% group_by(channel) %>%
  summarize(total_populism = sum(populism_count),
            total_environment = sum(environment_count, na.rm = T),
            total_immigration = sum(immigration_count, na.rm = T),
            total_proggresivsim = sum(progressive_count, na.rm = T),
            total_conservatism = sum(conservatism_count, na.rm = T),
            .groups = "drop")

summ_table_gt <- sum_table %>%
  gt() %>%
  tab_header(title = "Language Sentiment Across Channels",
             subtitle = "October 17 to November 14")



```


```{r regression}

sentiment_channels

populist_fit <- lm(populism_count ~ ranking, data = sentiment_channels)
immigration_fit <- lm(immigration_count ~ ranking, data = sentiment_channels)
env_fit <- lm(environment_count ~ ranking, data = sentiment_channels)
prog_fit <- lm(progressive_count ~ ranking, data = sentiment_channels)
cons_fit <- lm(conservatism_count ~ ranking, data = sentiment_channels)
summary(ranking_fit)

stargazer(populist_fit, immigration_fit,
          env_fit,
          prog_fit,
          cons_fit,
          type = "text")


```


---
title: "gov_51_final_project"
author: "Owen Bernstein and Lindsey Greenhill"
date: "11/15/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Loading necessary packages

library(gt)
library(quanteda)
library(broom)
library(skimr)
library(lubridate)
library(janitor)
library(dotwhisker)
library(tidytext)
library(ggthemes)
library(webshot)
library(stargazer)
library(tidyverse)
library(patchwork)
library(ggrepel)
```

```{r data loading, include = FALSE}

# Reading in the data and then using bind rows to combine the candidates'
# Chyrons.

data <- read_csv("data/10-17-10-23.csv") %>%
  bind_rows(read_csv("data/10-24-10-31.csv")) %>%
  bind_rows(read_csv("data/11-1-11-7.csv")) %>%
  bind_rows(read_csv("data/11-8-11-14.csv")) %>%
  clean_names() %>%
  filter(channel != "BBCNEWS") %>%
  mutate(ranking = if_else(channel == "FOXNEWSW",
                           1, if_else(channel == "CNNW", 2, 3)))

# Cleaning the Chyrons and creating variables for post election and primetime
# coverage

tidy_data <- data %>% 
  clean_names() %>% 
  mutate(date = mdy_hm(date_time_utc)) %>%
  mutate(date_x = as.Date(substr(date, 1, 10)),
         hour = as.double(substr(date, 12, 13))) %>%
  select(channel, text, ranking, date_x, hour) %>% 
  mutate(post_election = ifelse(date_x > "2020-11-03", "Post-Election", "Pre-Election"),
         primetime = ifelse(hour > 19 & hour < 24, "Primetime", "Not Primetime"))

# Turning data into a corpus for quanteda

text_corpus <- corpus(tidy_data, text_field = "text")

```

```{r wordcloud, include = FALSE}

# Turning corpus into tokens. Removing unimportant words and selecting for
# ngrams = 2

wordcloud_toks <- tokens(text_corpus, 
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE,
                      remove_url = TRUE) %>% 
  tokens_tolower() %>%
  tokens_remove(pattern=stopwords("en")) %>% 
  tokens_remove(pattern= c("u201c", "u00b0", "u2014", "wopi", "avi", "ooo", "000", "ito", "ynl", "f'avl", "foxnews.com", "ufb02pi", "ufb021", "ufbo2l", "ufb02L", "rpm")) %>% 
  tokens_select(min_nchar=3) %>% 
  tokens_ngrams(n = 2)

wordcloud_dfm <- dfm(wordcloud_toks, groups = "channel")

textplot_wordcloud(wordcloud_dfm, comparison = T, min_count = 150)

```

```{r comparison graphs, include = FALSE}

# Using tokens to compare word usage by post_election variable

post_election_dfm <- dfm(wordcloud_toks, groups = "post_election")

post_election_keyness <- textstat_keyness(post_election_dfm, target = "Post-Election")

post_election_relative <- textplot_keyness(post_election_keyness, n = 15L)

# Using tokens to compare word usage by primetime variable

primetime_dfm <- dfm(wordcloud_toks, groups = "primetime")

primetime_keyness <- textstat_keyness(primetime_dfm, target = "Primetime")

primetime_relative <- textplot_keyness(primetime_keyness, n = 15L)

```

```{r dictionary and data frame, include = FALSE}

# Creating the content dictionaries

content_dict <- dictionary(list(populism = c("deceit", "treason",
                             "betray", "absurd",
                             "arrogant", "promise", 
                             "corrupt", "direct",
                             "elite", "establishment",
                             "ruling", "caste",
                             "class", "mafia",
                             "undemocratic", "politics", "political", "politicize", "politician",
                             "propaganda", "referendum",
                             "regime", "shame",
                             "admit", "tradition",
                             "people"),
                environment = c("green","climate",
                                "environment","heating",
                                "durable"),
                immigration = c("asylum","halal",
                                "scarf","illegal",
                                "immigrant", "immigration", "immigrate", "Islam", 
                                "Koran","Muslim",
                                "foreign"),
                progressive = c("progress","right",
                                "freedom","self-disposition",
                                "handicap","poverty",
                                "protection","honest",
                                "equal","education",
                                "pension","social",
                                "weak"),
                conservative = c("belief","famil",
                                 "church","norm",
                                 "porn","sex",
                                 "values","conservative",
                                 "conservatism","custom")))

# Creating new dfm for the content analysis

content_toks <- tokens(text_corpus, 
                      remove_punct = TRUE,
                      remove_symbols = TRUE,
                      remove_numbers = TRUE,
                      remove_url = TRUE) %>% 
  tokens_tolower() %>%
  tokens_remove(pattern=stopwords("en")) %>% 
  tokens_select(min_nchar=3)

content_dfm <- dfm(content_toks, groups = c("channel", "date_x"))

# Selecting words in the dictionaries

content_categories <- dfm_lookup(content_dfm, dictionary = content_dict)

# Turning dfm into dataframe

content_df <- convert(content_categories, to = "data.frame") %>% 
  group_by(doc_id) %>% 
  separate(doc_id, c("channel", "date"), extra = "merge") %>% 
  mutate(date = ymd(date))

```

```{r creating boxplots, include = FALSE}

# Creating boxplot for populism by channel

pop_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, populism)) +
  geom_boxplot() + 
  labs(title = "Populist Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

# Creating boxplot for environment by channel

env_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, environment)) +
  geom_boxplot() + 
  labs(title = "Environmental Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

# Creating boxplot for immigration by channel

img_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, immigration)) +
  geom_boxplot() + 
  labs(title = "Immigration Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

# Creating boxplot for progressive by channel

pro_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, progressive)) +
  geom_boxplot() + 
  labs(title = "Progressive Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

# Creating boxplot for conservative by channel

con_box <- content_df %>% 
  group_by(channel) %>% 
  ggplot(., aes(channel, conservative)) +
  geom_boxplot() + 
  labs(title = "Conservative Language in Chyrons by Channel",
       subtitle = "October 17 to November 14",
       x = "Channel", y = "Language Count") +
  theme_minimal() +
  theme(axis.line = element_line())

```

```{r making gt table, include = FALSE}

# Making gt table of full counts

full_count_dfm <- dfm(content_toks, groups = c("channel"))

full_count_categories <- dfm_lookup(full_count_dfm, dictionary = content_dict)

full_count_df <- convert(full_count_categories, to = "data.frame") %>% 
  mutate(Channel = doc_id) %>% 
  select(-doc_id)

full_count_gt <- gt(full_count_df) %>% 
  tab_header(title = "Language Sentiment Across Channels",
             subtitle = "October 17 to November 14") %>% 
  cols_move(
    columns = vars(populism, environment, immigration, progressive, conservative),
    after = vars(Channel)
  ) %>% 
  tab_spanner(label = "Word Count for Each Content Category", columns = vars(populism, environment, immigration, progressive, conservative)) %>% 
  cols_label(populism = "Populism", environment = "Environment", immigration = "Immigration", 
             progressive = "Progressive", conservative = "Conservative")


```

```{r regressions, include = FALSE}

# Making dfm for regression and turning it into a df

regression_dfm <- dfm(content_toks, groups = c("channel", "post_election", "primetime", "date_x", "ranking"))

regression_categories <- dfm_lookup(regression_dfm, dictionary = content_dict)

regression_df <- convert(regression_categories, to = "data.frame") %>% 
  group_by(doc_id) %>% 
  separate(doc_id, c("channel", "election", "primetime", "date", "ranking"), sep = "([.])", extra = "merge") %>% 
  mutate(date = ymd(date))

# Running regressions

pop_fit <-
  lm(populism ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)

env_fit <-
  lm(environment ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)

img_fit <-
  lm(immigration ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)

pro_fit <-
  lm(progressive ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)

con_fit <-
  lm(conservative ~ as.factor(channel) + as.factor(election) + as.factor(primetime),
     data = regression_df)
```

## Preliminary Analysis

In this analysis, we gathered, cleaned, and wrangled news data from October 17 to November 14, 2020. 
We chose to include Fox News, CNN, and MSNB in our analysis and assigned those channels ideological
rankings of 1, 2, and 3, respectively. The plots below show our preliminary findings.

### How much specific language is each channel using?

Visualizing the language each channel uses by a variety of variables. 

```{r visualizations, echo = FALSE}

textplot_wordcloud(wordcloud_dfm, comparison = T, min_count = 150)

post_election_relative

primetime_relative

```


We classified the below categories using word baskets built by the study referenced in our project proposal. The table below shows the raw counts of how many times each channel used a word associated with these categories. 

```{r gt, echo=FALSE}
full_count_gt
```

### How much specific language is each channel using each day?

The histograms below visualize how much channels use the categorical language on a daily basis. 

```{r hists, warning = FALSE, echo=FALSE}

pop_box
img_box  
env_box 
pro_box 
con_box

```

### How does usage vary across time?

The plots below visualize the change in daily language use for each channel.

```{r time, echo=FALSE, warning=FALSE}
#language_all_counts
```

### Regression: Does Ideology have an affect on language usage?

The table below shows the results of five different regressions and the plot below shows the line of best fit for each regression. 

```{r regs, results = "asis", echo=FALSE, warning=FALSE}
stargazer(pop_fit, img_fit,
          env_fit,
          pro_fit,
          con_fit,
          single.row = TRUE,
          column.sep.width = "0.3pt",
          font.size= "footnotesize",
          type = "latex")

```

There does not appear to be a strong relationship between ideology and language usage. 

